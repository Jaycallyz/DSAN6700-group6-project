{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a1c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Basic file saved: ../../clean_data/destination_sample_basic.csv (150 rows)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=6b4fa9620273c59333d6be7f63aee08eeb7335898093fc34f3d2c97b0e8d42a2\n",
      "  Stored in directory: /Users/zhouyiqin/Library/Caches/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouyiqin/miniconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/zhouyiqin/miniconda3/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wikipedia file saved: ../../clean_data/destination_sample_wikipedia.csv (150 rows)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "INPUT_PATH = Path(\"../../raw_data/worldcitiespop.csv\")\n",
    "OUTPUT_DIR = Path(\"../../clean_data\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_BASIC = OUTPUT_DIR / \"destination_sample_basic.csv\"\n",
    "OUT_WIKI  = OUTPUT_DIR / \"destination_sample_wikipedia.csv\"\n",
    "\n",
    "usecols = [\"Country\", \"City\", \"AccentCity\", \"Region\", \"Population\", \"Latitude\", \"Longitude\"]\n",
    "df = pd.read_csv(INPUT_PATH, encoding=\"latin-1\", usecols=usecols, low_memory=False)\n",
    "\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "df = df.rename(columns={\"country\": \"country_code\", \"accentcity\": \"city_display\"})\n",
    "\n",
    "for c in [\"city\", \"city_display\", \"country_code\", \"region\"]:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "df[\"population\"] = pd.to_numeric(df[\"population\"], errors=\"coerce\")\n",
    "df[\"latitude\"]   = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "df[\"longitude\"]  = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "\n",
    "df = df[(df[\"city\"].str.len() > 0) & df[\"latitude\"].notna() & df[\"longitude\"].notna()].copy()\n",
    "\n",
    "df[\"name\"] = np.where(df[\"city_display\"].str.len() > 0, df[\"city_display\"], df[\"city\"].str.title())\n",
    "\n",
    "df[\"country\"] = df[\"country_code\"].str.upper()\n",
    "\n",
    "df = df.sort_values([\"population\"], ascending=False)\n",
    "df = df.drop_duplicates(subset=[\"name\", \"country\"])\n",
    "\n",
    "TARGET_N = 150\n",
    "MAX_PER_COUNTRY = 5\n",
    "\n",
    "selected_rows = []\n",
    "country_counts = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    ctry = row[\"country\"]\n",
    "    if country_counts.get(ctry, 0) >= MAX_PER_COUNTRY:\n",
    "        continue\n",
    "    selected_rows.append(row)\n",
    "    country_counts[ctry] = country_counts.get(ctry, 0) + 1\n",
    "    if len(selected_rows) >= TARGET_N:\n",
    "        break\n",
    "\n",
    "sel = pd.DataFrame(selected_rows)\n",
    "\n",
    "\n",
    "if len(sel) < TARGET_N:\n",
    "    remaining = df[~df.index.isin(sel.index)]\n",
    "    extra = remaining.head(TARGET_N - len(sel))\n",
    "    sel = pd.concat([sel, extra], ignore_index=True)\n",
    "\n",
    "def make_basic_desc(row):\n",
    "    parts = []\n",
    "    parts.append(f\"{row['name']} is a city in {row['country']}.\")\n",
    "    if pd.notna(row[\"population\"]) and row[\"population\"] > 0:\n",
    "        parts.append(f\"It has an estimated population of about {int(row['population']):,} people.\")\n",
    "    if isinstance(row.get(\"region\", \"\"), str) and row[\"region\"].strip():\n",
    "        parts.append(f\"It is located in region {row['region']}.\")\n",
    "    if pd.notna(row[\"latitude\"]) and pd.notna(row[\"longitude\"]):\n",
    "        lat = row[\"latitude\"]; lng = row[\"longitude\"]\n",
    "        hemi_ns = \"northern\" if lat >= 0 else \"southern\"\n",
    "        hemi_ew = \"eastern\"  if lng >= 0 else \"western\"\n",
    "        parts.append(f\"The city lies in the {hemi_ns} and {hemi_ew} hemispheres.\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "sel[\"description\"] = sel.apply(make_basic_desc, axis=1)\n",
    "\n",
    "basic_cols = [\"name\",\"country\",\"city\",\"region\",\"population\",\"latitude\",\"longitude\",\"description\"]\n",
    "basic_cols = [c for c in basic_cols if c in sel.columns]\n",
    "sel[basic_cols].to_csv(OUT_BASIC, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Basic file saved: {OUT_BASIC} ({len(sel)} rows)\")\n",
    "\n",
    "# ---------------- Wikipedia ----------------\n",
    "USE_WIKIPEDIA = True\n",
    "\n",
    "if USE_WIKIPEDIA:\n",
    "    try:\n",
    "        try:\n",
    "            import wikipedia\n",
    "        except Exception:\n",
    "            import sys, subprocess\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"wikipedia\"], check=True)\n",
    "            import wikipedia\n",
    "\n",
    "        wikipedia.set_lang(\"en\")\n",
    "\n",
    "        def clean_wiki_summary(text):\n",
    "            if not isinstance(text, str):\n",
    "                return \"\"\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "            return text\n",
    "\n",
    "        wiki_desc = []\n",
    "        for i, row in sel.iterrows():\n",
    "            name = row[\"name\"]\n",
    "            country = row[\"country\"]\n",
    "            summary = \"\"\n",
    "            queries = [f\"{name}, {country}\", name]\n",
    "            for q in queries:\n",
    "                try:\n",
    "                    summary = wikipedia.summary(q, sentences=2, auto_suggest=True, redirect=True)\n",
    "                    summary = clean_wiki_summary(summary)\n",
    "                    if summary:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    summary = \"\"\n",
    "            if not summary:\n",
    "                summary = row[\"description\"]\n",
    "            wiki_desc.append(summary)\n",
    "            time.sleep(0.35)\n",
    "\n",
    "        sel[\"description\"] = wiki_desc\n",
    "        sel[basic_cols].to_csv(OUT_WIKI, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[OK] Wikipedia file saved: {OUT_WIKI} ({len(sel)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Wikipedia enrichment failed:\", repr(e))\n",
    "        print(\"Kept the basic file only.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
